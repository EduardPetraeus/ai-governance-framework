# Dual-Model Validation

## Name

Dual-Model Validation — route written work to a different model for review.

## Problem

The same model reviewing its own work has the same blind spots as when it wrote it. A model reviewing code it generated will miss approximately the same types of errors it made when generating it. This is not a theoretical concern — it is a structural limitation of self-review.

When Sonnet generates a retry function with an off-by-one error in the backoff calculation, Sonnet reviewing that same function will evaluate the logic using the same reasoning patterns that produced the error. The review becomes a confirmation ritual rather than an independent check.

Self-review catches superficial issues: typos, missing imports, obvious syntax errors. It does not catch the errors that matter most: logic flaws, missed edge cases, incorrect assumptions about API behavior, and architectural decisions that seem reasonable in isolation but conflict with the broader system design.

## Solution

Systematically route written work to a different model for review. The writing model and reviewing model must be different — ideally with the reviewing model being stronger or having different reasoning characteristics.

```
Task → Sonnet writes code → Opus reviews code → Issues found? → Fix → Merge
                                                → No issues?   → Merge
```

The asymmetry in model capabilities means the reviewing model catches classes of errors that the writing model's self-review misses. Opus reasons more carefully about edge cases, security implications, and architectural fit. Sonnet is faster and cheaper for implementation. Using both in sequence gives you speed on the write side and depth on the review side.

## When to Use

- All production code changes generated by AI agents
- Architecture decisions documented by AI agents
- Security-sensitive code (authentication, authorization, encryption)
- Changes to governance files (CLAUDE.md, CI/CD workflows)
- Any output where the cost of an undetected error exceeds the cost of the review

## When NOT to Use

- Internal documentation updates with no code implications
- CHANGELOG entries and session summaries
- Formatting-only changes (linters handle this better and cheaper)
- Exploratory prototypes that will be discarded
- When the reviewing model is the same as the writing model (this provides no benefit — use a different quality pattern instead)

## Implementation

### Step 1: Define model roles

Assign models to roles based on their strengths:

| Role | Recommended Model | Rationale |
|------|------------------|-----------|
| Implementation (writing) | Sonnet | Fast, cost-effective, strong at code generation |
| Review (reading) | Opus | Stronger reasoning, better at edge cases and architectural analysis |
| Quick fixes and formatting | Haiku | Fastest, cheapest, sufficient for mechanical changes |

### Step 2: Configure review triggers in CLAUDE.md

Add to your project's CLAUDE.md:

```yaml
automatic_review_triggers:
  - trigger: "PR created from sonnet-session branch"
    reviewer: "opus"
    focus: ["logic correctness", "edge cases", "security implications", "architectural fit"]
  - trigger: "changes to CI/CD workflows"
    reviewer: "opus"
    focus: ["security", "pipeline correctness", "governance compliance"]
  - trigger: "changes to CLAUDE.md or governance files"
    reviewer: "opus"
    focus: ["consistency", "completeness", "no weakening of existing rules"]
```

### Step 3: Add to CI/CD pipeline

Create a GitHub Actions workflow that triggers an Opus review on PRs from Sonnet sessions:

```yaml
name: Dual-Model Review
on:
  pull_request:
    branches: [main]

jobs:
  opus-review:
    if: contains(github.head_ref, 'sonnet-session')
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Run Opus review
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          # Collect diff for review
          git diff origin/main...HEAD > changes.diff
          # Submit to Opus for review with governance context
          python scripts/dual_model_review.py \
            --diff changes.diff \
            --claude-md CLAUDE.md \
            --model claude-opus-4-20250514
```

### Step 4: Define review focus areas

The reviewing model should focus on what automated tools cannot check:

1. **Logic correctness** — Does the code do what the task requires? Are calculations correct?
2. **Edge cases** — What happens with empty input, maximum values, concurrent access, network failures?
3. **Security implications** — Are credentials handled safely? Is input validated? Are permissions checked?
4. **Architectural fit** — Does this change align with existing patterns? Does it introduce coupling?
5. **Contract compliance** — Does the output match the [output contract](output-contracts.md) for this task?

### Step 5: Handle review findings

Review findings should be actionable:

```yaml
review_finding:
  severity: high | medium | low
  location: "file.py:42-58"
  issue: "Retry backoff calculation uses multiplication instead of exponentiation"
  suggestion: "Replace `delay * attempt` with `delay * (2 ** attempt)`"
  confidence: 92%
```

High-severity findings block the merge. Medium-severity findings require acknowledgment. Low-severity findings are informational.

## Example

A Sonnet session creates a new API connector for pulling sleep data from the Oura Ring API. The session produces:

- `source_connectors/oura/sleep.py` — the connector implementation
- `tests/test_oura_sleep.py` — unit tests
- Updated `sources_config.yaml` — connector registration

The code looks syntactically correct. All tests pass. The linter reports no issues.

An Opus review session examines the PR and identifies:

1. **Off-by-one in backoff calculation** (high severity): The retry logic calculates delay as `base_delay * retry_count` instead of `base_delay * (2 ** retry_count)`. On the 5th retry, the delay is 5 seconds instead of 32 seconds. This could trigger API rate limiting.

2. **Missing Retry-After header handling** (high severity): When the API returns HTTP 429, it includes a `Retry-After` header specifying the wait time. The connector ignores this header and uses its own backoff calculation, which may be shorter than the API requires.

3. **UTC assumption not validated** (medium severity): The connector assumes all timestamps from the Oura API are in UTC. The API documentation states timestamps are in the user's local timezone unless explicitly requested in UTC. The `timezone` parameter is not being set in the API request.

None of these issues would be caught by linting, type checking, or the test suite (which tests against mock responses that happen to be in UTC). All three would cause production bugs.

## Evidence

Dual-model validation works because of a well-established principle in quality assurance: independent review catches errors that self-review misses. This is the same principle behind code review in software engineering, peer review in academic publishing, and auditing in financial accounting.

The specific advantage of using different AI models (rather than the same model twice) is that different models have genuinely different reasoning patterns. They are not two humans with the same training — they are systems with different architectures, training data emphasis, and reasoning characteristics. This structural difference produces meaningfully independent review.

In practice, teams using dual-model validation report catching 2-3 additional significant issues per 10 PRs compared to single-model review. The cost of the additional review (one Opus API call per PR) is a fraction of the cost of a production bug.

## Related Patterns

- [Output Contracts](output-contracts.md) — defines what the review checks against
- [Progressive Trust](progressive-trust.md) — dual-model review can be relaxed as trust increases
- [Semantic Verification](semantic-verification.md) — complements model review with structured verification techniques
- [Human-in-the-Loop](human-in-the-loop.md) — human review is Layer 4; dual-model validation is Layer 3
- [Quality Control Patterns](../docs/quality-control-patterns.md) — how this pattern fits the quality control stack
